# Grok Assistant

A chat application built with Python (FastAPI backend) and Next.js (frontend) that uses Letta for agent loops and memory management. The application supports multiple personas (personal assistant, dating coach, friend, etc.) and remembers conversations across sessions.

## Features

- ü§ñ **Agent Loop**: Powered by Letta SDK for intelligent conversation handling
- üß† **Memory Management**: Automatic conversation summarization and context retention
- üë§ **Multiple Personas**: Switch between different AI personas (personal assistant, dating coach, friend)
- üí¨ **Conversation Memory**: Remembers past interactions and maintains context
- üåê **Web Interface**: Modern Next.js frontend with FastAPI backend
- üîå **REST API**: Full REST API for integration and extensibility

## Prerequisites

- Python 3.12 or higher
- [uv](https://github.com/astral-sh/uv) (fast Python package installer)
- Node.js 18+ and npm (for frontend)
- Docker (for running Letta server)
- Ollama (for free embeddings)
- xAI API key (for Grok models)

## Installation

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd grok-assistant
```

### 2. Backend Setup

**a. Create virtual environment and install dependencies:**

```bash
# Using uv sync (recommended - automatically creates venv and installs dependencies)
uv sync

# Or manually:
uv venv --python 3.12
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r backend/requirements.txt
```

**b. Set up environment variables:**

Create a `.env` file in the project root:

```bash
# Letta server URL (defaults to localhost:8283)
LETTA_API_URL=http://localhost:8283

# Model configuration
LETTA_MODEL=xai/grok-3-mini
XAI_API_KEY=your_xai_api_key_here

# Ollama configuration (for free embeddings)
OLLAMA_API_URL=http://localhost:11434
LETTA_OLLAMA_EMBEDDING_MODEL=all-minilm

# API server port (optional, defaults to 8000)
API_PORT=8000
```

**c. Install and start Ollama (for free embeddings):**

```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service
brew services start ollama

# Pull an embedding model
ollama pull all-minilm
```

**d. Start the Letta server with Docker:**

```bash
# If a container already exists, stop and remove it first:
docker stop letta-server 2>/dev/null && docker rm letta-server 2>/dev/null

# Start the Letta server
docker run -d --name letta-server \
  -v ~/.letta/.persist/pgdata:/var/lib/postgresql/data \
  -p 8283:8283 \
  -e XAI_API_KEY=your_xai_api_key_here \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  letta/letta:latest
```

**Note:** To restart the server later:
```bash
docker restart letta-server
```

### 3. Frontend Setup

```bash
cd frontend
npm install
```

**Optional:** Create `frontend/.env.local` to customize API URL:
```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
```

## Running the Application

### Quick Start (Full Stack)

You need **two terminal windows**:

**Terminal 1 - Backend:**
```bash
cd /path/to/grok-assistant
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
python backend/run_server.py
```

The backend will start on `http://localhost:8000` with API docs at `http://localhost:8000/docs`.

**Terminal 2 - Frontend:**
```bash
cd /path/to/grok-assistant/frontend
npm run dev
```

The frontend will start on `http://localhost:3000`.

**Open your browser:** Navigate to `http://localhost:3000` to use the application.

### CLI Application (Backend Only)

You can also run the backend as a CLI application:

```bash
# From the project root
python -m backend.chat_app

# Or if you're in the backend directory
cd backend
python chat_app.py
```

**Available Commands:**
- **Regular chat**: Just type your message and press Enter
- **`set_persona <name>`**: Switch to a different persona
  - Available personas: `personal_assistant`, `dating_coach`, `friend`
- **`memory`**: View the current conversation memory
- **`personas`**: List all available personas
- **`exit`** or **`quit`**: Exit the application

## API Documentation

The FastAPI backend provides REST API endpoints for frontend integration and external use.

### Base URL
- **Development**: `http://localhost:8000`
- **Interactive Docs**: `http://localhost:8000/docs`

### Endpoints

#### Health Check
- `GET /health` - Check server status and configuration

#### Personas
- `GET /api/personas` - Get all available personas
- `GET /api/personas/current` - Get the current active persona
- `POST /api/personas/set` - Set the active persona
  ```json
  {
    "persona_key": "personal_assistant"
  }
  ```

#### Chat
- `POST /api/chat` - Send a chat message and get a response
  ```json
  {
    "message": "Hello! How are you?",
    "conversation_id": "conv-123"
  }
  ```
  - `conversation_id`: **Required** - Identifies which conversation this message belongs to

#### Conversation History
- `GET /api/conversation/history` - Get conversation history for a specific conversation
  - Query parameters:
    - `conversation_id` (required): The conversation ID to fetch history for
    - `limit` (optional): Maximum number of messages to retrieve
    - `order` (optional): `'asc'` for oldest first, `'desc'` for newest first (default: `'desc'`)

#### Memory
- `GET /api/memory` - Get the agent's memory blocks

#### Agent Information
- `GET /api/agent/id` - Get the current agent ID (for debugging)

### Example API Calls

```bash
# Health check
curl http://localhost:8000/health

# Get personas
curl http://localhost:8000/api/personas

# Send a chat message
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!", "conversation_id": "conv-123"}'

# Get conversation history
curl "http://localhost:8000/api/conversation/history?conversation_id=conv-123&limit=10&order=desc"
```

## Understanding IDs and Architecture

### Conversation ID (`conversation_id`)
- **Primary identifier** - Use this to identify and fetch conversations
- Each conversation has a unique `conversation_id` (you provide this, e.g., `conv-123` or UUID)
- Pass `conversation_id` when sending messages to maintain conversation context
- Use `conversation_id` to fetch messages from a specific conversation

### Agent ID (`agent_id`)
- **Shared across conversations** - One agent per persona, not per conversation
- Created when you set a persona (e.g., one agent for "personal_assistant")
- **All conversations with the same persona share the same agent**
- Used internally by Letta - you don't need separate agents for each conversation
- Conversations are separated by `conversation_id`/`group_id`, not by `agent_id`

**Example:**
```
Persona: "personal_assistant"
  ‚îî‚îÄ‚îÄ Agent ID: agent-xxx (ONE agent)
      ‚îú‚îÄ‚îÄ Conversation 1 (conv-123) ‚Üí Messages filtered by group_id
      ‚îú‚îÄ‚îÄ Conversation 2 (conv-456) ‚Üí Messages filtered by group_id
      ‚îî‚îÄ‚îÄ Conversation 3 (conv-789) ‚Üí Messages filtered by group_id
```

### Group ID (`group_id`)
- **Letta's internal parameter** - Used for filtering messages in Letta's API
- You typically don't need to use `group_id` directly - just use `conversation_id` and the backend handles the mapping
- The backend maps your `conversation_id` to Letta's `group_id` when fetching history

## Available Models

- `xai/grok-3-mini` - Fast and cost-effective (recommended)
- `xai/grok-2-1212` - Latest full model
- Other xAI/Grok models supported by Letta

The application will automatically use the model specified in `LETTA_MODEL` when creating agents.

## Personas

### Personal Assistant
A helpful and efficient assistant for daily tasks, scheduling, and productivity.

### Dating Coach
An empathetic coach offering advice on dating and relationships.

### Friend
A casual, friendly companion for everyday conversation.

## Project Structure

```
grok-assistant/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # Backend package initialization
‚îÇ   ‚îú‚îÄ‚îÄ chat_app.py          # Main chat application with CLI interface
‚îÇ   ‚îú‚îÄ‚îÄ personas.py          # Persona configurations and management
‚îÇ   ‚îú‚îÄ‚îÄ api.py               # FastAPI REST API server
‚îÇ   ‚îú‚îÄ‚îÄ run_server.py        # Script to run the FastAPI server
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ AGENT_ARCHITECTURE.md # Agent architecture documentation
‚îÇ   ‚îî‚îÄ‚îÄ .env.example         # Environment variables template
‚îú‚îÄ‚îÄ frontend/                # Next.js frontend application
‚îÇ   ‚îú‚îÄ‚îÄ app/                 # Next.js app directory
‚îÇ   ‚îú‚îÄ‚îÄ components/          # React components
‚îÇ   ‚îú‚îÄ‚îÄ hooks/               # React hooks
‚îÇ   ‚îú‚îÄ‚îÄ lib/                 # Utilities and API client
‚îÇ   ‚îî‚îÄ‚îÄ types/               # TypeScript type definitions
‚îú‚îÄ‚îÄ .env                     # Environment variables (not in git)
‚îú‚îÄ‚îÄ .gitignore              # Git ignore rules
‚îî‚îÄ‚îÄ README.md               # This file
```

## Troubleshooting

### Backend Issues

**Backend won't start:**
- Check if Letta server is running: `docker ps | grep letta`
- Check if port 8000 is available: `lsof -i :8000`
- Verify `.env` file exists and has correct values
- Check backend logs for errors

**Letta connection errors:**
- Verify Letta server is running: `docker ps | grep letta`
- Check `LETTA_API_URL` in `.env` matches your Docker container port
- Restart Letta server: `docker restart letta-server`

**Embedding model errors:**
- Ensure Ollama is running: `brew services list | grep ollama`
- Verify embedding model is pulled: `ollama list | grep all-minilm`
- Check `OLLAMA_API_URL` and `LETTA_OLLAMA_EMBEDDING_MODEL` in `.env`

### Frontend Issues

**Frontend won't start:**
- Run `npm install` in the frontend directory
- Check if port 3000 is available: `lsof -i :3000`
- Check Node.js version: `node --version` (should be 18+)

**Frontend can't connect to backend:**
- Verify backend is running: `curl http://localhost:8000/health`
- Check CORS settings in `backend/api.py`
- Verify `NEXT_PUBLIC_API_URL` in `frontend/.env.local` (if set)
- Check browser console for detailed error messages

**"Setting up assistant" stuck:**
- Check browser console for errors
- Verify backend is running and accessible
- Check that persona API endpoint is working: `curl http://localhost:8000/api/personas/current`

### General Issues

**Port already in use:**
```bash
# Find and kill process on port 8000 (backend)
lsof -ti:8000 | xargs kill -9

# Find and kill process on port 3000 (frontend)
lsof -ti:3000 | xargs kill -9
```

**Docker container issues:**
```bash
# Stop and remove container
docker stop letta-server
docker rm letta-server

# View logs
docker logs letta-server

# Restart container
docker restart letta-server
```

## CORS Configuration

The API is configured to accept requests from:
- `http://localhost:3000` (Next.js default)
- `http://localhost:3001`
- `http://127.0.0.1:3000`
- `http://127.0.0.1:3001`

To add more origins, edit `backend/api.py` and update the `allow_origins` list in the CORS middleware configuration.

## Memory Management

The application uses Letta's memory management system:
- **Working Memory**: Retains the last 20 messages by default
- **Summarization**: Automatically summarizes conversations after 15 messages
- **Context Retention**: Maintains conversation context across interactions

## Debugging

### Viewing Letta Memory and Debug Information

You can use Letta's native client methods directly for debugging. The existing API endpoints already use these methods:

#### 1. View Memory Blocks (via API)
```bash
curl http://localhost:8000/api/memory
```

This uses Letta's `client.agents.blocks.list()` method internally.

#### 2. View Conversation History (via API)
```bash
curl "http://localhost:8000/api/conversation/history?conversation_id=conv-123&limit=50"
```

This uses Letta's `client.agents.messages.list()` method internally.

#### 3. Get Agent ID (via API)
```bash
curl http://localhost:8000/api/agent/id
```

#### 4. Using Letta Client Directly (Python)

You can also use Letta's Python client directly for more detailed debugging:

```python
from letta_client import Letta
import os

# Initialize Letta client
client = Letta(base_url=os.getenv("LETTA_API_URL", "http://localhost:8283"))

# Get agent ID (from your app)
agent_id = "your-agent-id"  # Get from /api/agent/id

# View memory blocks (Letta native method)
memory_blocks = client.agents.blocks.list(agent_id=agent_id)
for block in memory_blocks:
    print(f"Label: {block.label}")
    print(f"Value: {block.value}")
    print()

# View all messages (Letta native method)
messages = client.agents.messages.list(
    agent_id=agent_id,
    limit=100,
    order="desc",
    order_by="created_at"
)
for msg in messages:
    print(f"Type: {msg.message_type}")
    print(f"Content: {msg.content}")
    print()

# Get agent details (Letta native method)
agent = client.agents.get(agent_id=agent_id)
print(f"Agent: {agent.name}")
print(f"Model: {agent.model}")
print(f"System: {agent.system}")
```

#### 5. Direct Letta API Access (HTTP)

You can also access Letta's REST API directly:

```bash
# Get agent ID first
AGENT_ID=$(curl -s http://localhost:8000/api/agent/id | jq -r '.agent_id')

# Get agent details
curl "http://localhost:8283/v1/agents/${AGENT_ID}"

# List all messages for an agent
curl "http://localhost:8283/v1/agents/${AGENT_ID}/messages?limit=100"

# List memory blocks
curl "http://localhost:8283/v1/agents/${AGENT_ID}/blocks"
```

#### 6. Interactive API Documentation

The easiest way to explore available endpoints:
- Open `http://localhost:8000/docs` in your browser
- Try out the endpoints interactively

### Common Debugging Scenarios

**Problem: Messages not appearing in conversation**
```bash
# Check conversation history via API
curl "http://localhost:8000/api/conversation/history?conversation_id=your-conv-id"

# Or use Letta client directly in Python to see all messages including system ones
```

**Problem: Memory not being stored**
```bash
# Check memory blocks via API
curl http://localhost:8000/api/memory

# Or use Letta client: client.agents.blocks.list(agent_id=agent_id)
```

**Problem: Need to see system messages for debugging**
Use Letta's client directly in Python - the API filters out system messages for the UI, but you can access them via the Letta client.

## Development

### Stopping Services

- **Backend**: Press `Ctrl+C` in Terminal 1
- **Frontend**: Press `Ctrl+C` in Terminal 2
- **Letta**: `docker stop letta-server`

## Future Enhancements

- User authentication and session management
- Database storage for conversation history
- Additional personas
- Custom persona creation
- Multi-user support
- Conversation export/import

## License

MIT License

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
